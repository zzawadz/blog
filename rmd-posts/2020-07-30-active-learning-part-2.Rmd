---
title: Active learning - part 2
author: Zygmunt Zawadzki
date: '2020-07-19'
slug: active-learning-part-2
tags:
  - r
  - active-learning
  - machine-learning
  - rpkg-FSelectorRcpp
---

Second post in the 'Active Learning' series. This time I will explore the the 'exploration vs exploitation' concept.

<!--more-->

Let's start with some intuitions. Each classification algorithm relies on some relations between variables and predicted classes. This is more or less clear. The stronger the relationship, the better (more accurate the model is). Now lets assume that we have a sample of `N` observations. Based on that sample we should be able to estimate a variable importance (e.g. from random forest or using other algorithm, I'll be using `information_gain` from `FSelectorRcpp` package).

```{r}
library(FSelectorRcpp)
library(dplyr)

set.seed(123)
ir20 <- iris %>% sample_n(20)
information_gain(Species ~ ., ir20)
```
### Exploitation

The higher value the more important the variable is. From this sample it looks like the most important is the `Petal.Width`. Based on that knowledge we might construct an *active learning* algorithm with promotes this particular variable in the selection process. We found a signal, which might be a little bit noisy because of a small sample, so we want to do as much as possible to refine it as fast as possible. This might be a good strategy - to stick for something that works. However, in a longer run it might (and probably will) lead to local optimum and over-fitting.

### Exploration

On the opposite site there's random strategy where an algorithm explores the data space by random sampling, which might lead to discoveries of stronger and better relationships with target value. 
### Exploration vs Exploitation

Based on my intuition I think that those two are in a opposition - you cannot have both. The best possible scenario is to have a mixture which balances those two. It seems to be a similar idea that is behind *simulated annealing* where there's some probability of big jump to overcome current local optima. Of course in simulated annealing the chances of big jumps decreases over time but here it probably should increase because of diminishing return from exploitation of current best signal.

Of course the intuitions presented above are just intuitions and they're lacking of mathematical rigour. However I like to have some preconceptions and then compare them with formal theory and simulations. It makes the learning process much more fun and surprising (sometimes it blows my mind how naive my intuitions were).

```{r}
get_new_idx_entropy <- function(trainAll, train, fit, k) {

      tt <- suppressWarnings(train %>% select(-row_id) %>% discretize(y ~ .) %>% extract_discretize_transformer())
      dt <- trainAll %>% discretize_transform(disc = tt) %>% select(-y)
      tr <- discretize_transform(disc = tt, data = train) %>% select(-y)

      colNames <- head(colnames(dt),-1)
      res <- setNames(lapply(colNames, function(nm) {
        ent <- FSelectorRcpp:::fs_entropy1d(tr[[nm]])
        rr <- sapply(unique(dt[[nm]]), function(x) {
          setNames(FSelectorRcpp:::fs_entropy1d(c(tr[[nm]], x)) - ent, as.character(x))
        }, USE.NAMES = FALSE)

        rr <- tibble(nm = names(rr), value = rr)
        colnames(rr)[1] <- nm
        rr
      }), colNames)

      dt2 <- dt
      for(ii in res) {
        dt2 <- inner_join(dt2, ii, by = colnames(ii)[[1]])
      }

      dt2 <- dt2 %>% mutate(entrChange = rowSums(dt2 %>% select(contains("value"))))
      dt2 %>% arrange(desc(entrChange)) %>% select(row_id) %>% head(k)
}

######### entropy2
get_new_idx_entropy_information_gain <- function(trainAll, train, fit, k) {

  tt <- suppressWarnings(train %>% select(-row_id) %>% discretize(y ~ .) %>% extract_discretize_transformer())
  dt <- trainAll %>% discretize_transform(disc = tt) %>% select(-y)
  tr <- discretize_transform(disc = tt, data = train) %>% select(-y)

  colNames <- head(colnames(dt),-1)

  infGain <- information_gain(y ~ ., train %>% select(-row_id))

  res <- setNames(lapply(colNames, function(nm) {
    ent <- FSelectorRcpp:::fs_entropy1d(tr[[nm]])
    rr <- sapply(unique(dt[[nm]]), function(x) {
      setNames(FSelectorRcpp:::fs_entropy1d(c(tr[[nm]], x)) - ent, as.character(x))
    }, USE.NAMES = FALSE)

    rr <- rr * (infGain %>% filter(attributes == nm))$importance
    rr <- tibble(nm = names(rr), value = rr)
    colnames(rr)[1] <- nm
    rr
  }), colNames)

  dt2 <- dt
  for(ii in res) {
    dt2 <- inner_join(dt2, ii, by = colnames(ii)[[1]])
  }

  dt2 <- dt2 %>% mutate(entrChange = rowSums(dt2 %>% select(contains("value"))))
  dt2 %>% arrange(desc(entrChange)) %>% select(row_id) %>% head(k)
}
##########

entrRes <- pbapply::pbreplicate(20, make_active(trainAll, testAll, get_new_idx = get_new_idx_entropy))
set.seed(123)
entrResScaledByInformationGain <- pbapply::pbreplicate(20, make_active(trainAll, testAll, get_new_idx = get_new_idx_entropy_information_gain))

mostUncertain <- mostUncertain 
allRandom <- allRandom %>% transform_run %>% mutate(Type = "Random")
entrRes <- entrRes %>% transform_run %>% mutate(Type = "Entropy")
entrResScaledByInformationGain <- entrResScaledByInformationGain %>% transform_run %>% mutate(Type = "Entropy scaled by information gain")


```
