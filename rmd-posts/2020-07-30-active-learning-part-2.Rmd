---
title: Active learning - part 2
author: Zygmunt Zawadzki
date: '2020-07-19'
slug: active-learning-part-2
tags:
  - r
  - active-learning
  - machine-learning
  - rpkg-FSelectorRcpp
---

Second post in the 'Active Learning' series. This time I will explore the 'exploration vs exploitation' concept.

<!--more-->

```{r, message=FALSE, warning=FALSE}
library(knitr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(FSelectorRcpp)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, eval = TRUE)
```

Let's start with some intuitions. Each classification algorithm relies on some relations between variables and predicted classes. This is more or less clear. The stronger the relationship, the better (more accurate the model is). Now lets assume that we have a sample of `N` observations. Based on that sample we should be able to estimate a variable importance (e.g. from random forest or using other algorithm, I'll be using `information_gain` from `FSelectorRcpp` package).

```{r}
library(FSelectorRcpp)
library(dplyr)

set.seed(123)
ir20 <- iris %>% sample_n(20)
information_gain(Species ~ ., ir20)
```
### Exploitation

The higher value the more important the variable is. From this sample it looks like the most important is the `Petal.Width`. Based on that knowledge we might construct an *active learning* algorithm with promoting this variable in the selection process. We found a signal, which might be a bit noisy because of a small sample, so we want to do as much as possible to refine it as fast as possible. This might be a good strategy - to stick for something that works. However, in a longer run it might (and probably will) lead to local optimum and over-fitting.

### Exploration

On the opposite site there's random strategy where an algorithm explores the data space by random sampling, which might lead to discoveries of stronger and better relationships with target value.

### Exploration vs Exploitation

Based on my intuition, I think those two are in an opposition - you cannot have both. The best scenario is to have a mixture which balances those two. It seems to be a similar idea is behind *simulated annealing* where there's some probability of a big jump to overcome current local optima. In simulated annealing the chances of big jumps decreases over time, but here it probably should increase because of diminishing return from exploitation of current best signal.

The intuitions presented above are just intuitions, and they're lacking of mathematical rigour. However, I like to have some preconceptions and then compare them with formal theory and simulations. It makes the learning process much more fun and surprising (sometimes it blows my mind how naive my intuitions were).

## No more intuitions - let's do some simulations.

I'll use the same code that I used in the first post. So for more information about the simulation, please go there. The code is hidden to not pollute the post space.

In this simulation I will try to use an entropy to guide the new samples selecting process. In each round I'll select the samples for which the entropy gain is highest. See the examples below:


```{r}
xx <- c("x", "x", "x", "x", "y")
(currentEntropy <- FSelectorRcpp:::fs_entropy1d(xx))
(addX <- FSelectorRcpp:::fs_entropy1d(c(xx, "x")))
(addY <- FSelectorRcpp:::fs_entropy1d(c(xx, "y")))

# in this example the sample containing Y would be selected
addX - currentEntropy
addY - currentEntropy 
```
For the simplicity of the implementation I'll assume that all the variables are independent, so for evaluating a total entropy gain for a sample I'll just sum variables' individual entropies gains.

```{r}
# the code below looks a little bit complicated but it does what's expected :)
get_new_idx_entropy <- function(trainAll, train, fit, k) {

      # suppress warnings to hide warnings from discretize
      # which are rised when the discretization can't be performed
      tt <- suppressWarnings(train %>% select(-row_id) %>%
                               discretize(y ~ .) %>%
                               extract_discretize_transformer())
      
      # discretize the traingAll data - this is the search space in
      # which all the samples to select from resides.
      dt <- trainAll %>% discretize_transform(disc = tt) %>% select(-y)
      tr <- discretize_transform(disc = tt, data = train) %>% select(-y)

      
      colNames <- head(colnames(dt),-1) # remove row_id column - it's the last one
      
      # calculate the change in entropy by adding each value
      res <- setNames(lapply(colNames, function(nm) {
        ent <- FSelectorRcpp:::fs_entropy1d(tr[[nm]])
        rr <- sapply(unique(dt[[nm]]), function(x) {
          setNames(FSelectorRcpp:::fs_entropy1d(c(tr[[nm]], x)) - ent, as.character(x))
        }, USE.NAMES = FALSE)

        rr <- tibble(nm = names(rr), value = rr)
        colnames(rr)[1] <- nm
        rr
      }), colNames)
      
      # add change in entropy value to the main table
      dt2 <- dt
      for(ii in res) {
        dt2 <- inner_join(dt2, ii, by = colnames(ii)[[1]])
      }

      # sum entropy change for each row.
      dt2 <- dt2 %>% mutate(entrChange = rowSums(dt2 %>% select(contains("value"))))
      
      # select the rows with the biggest change.
      dt2 %>% arrange(desc(entrChange)) %>% select(row_id) %>% head(k)
}
```

```{r}
dataPath <- file.path("/tmp/data/", "bank-full.csv")
# read data and split into test/traings sets
library(readr)
allData <- readr::read_delim(dataPath, delim = ";", 
  col_types = readr::cols(
  age = col_double(),
  job = col_character(),
  marital = col_character(),
  education = col_character(),
  default = col_character(),
  balance = col_double(),
  housing = col_character(),
  loan = col_character(),
  contact = col_character(),
  day = col_double(),
  month = col_character(),
  duration = col_double(),
  campaign = col_double(),
  pdays = col_double(),
  previous = col_double(),
  poutcome = col_character(),
  y = col_character()
  ))

allData <- allData %>%
  mutate(row_id = row_number()) %>%
  select(-month, -job) %>% # I have a problem with those two columns during 
                           # training phase so I removed them
  mutate(y = y == "yes") # transform to TRUE/FALSE

# split the data into train/test sets.
set.seed(123)
idx <- sample.int(nrow(allData), nrow(allData)*0.2)

trainAll <- allData[-idx,]
testAll <- allData[idx,]

#' @param get_new_idx function which returns the selected rows
#' indexes to be labelled by the oracle. In this function the 'active learning'
#' logic resides.
make_active_learning_path <- function(trainAll, testAll, nstart = 500, n = 50, k = 50, get_new_idx) {

  # init training data set by selecting randomly nstart rows from 
  # 'unlabelled' data
  idxInit <- tibble(row_id = sample(trainAll$row_id, nstart))
  train <- inner_join(trainAll, idxInit, by = "row_id")
  trainAll <- anti_join(trainAll, idxInit, by = "row_id")

  aucRes <- rep(0, n)

  for(i in 1:n) {

    # build a classification model using simple logistics regression
    fit <- glm(y ~ ., data = train %>% select(-row_id), family = "binomial")

    # calculate AUC
    res <- predict(fit, newdata = testAll, type="response")
    aucRes[i] <- suppressMessages(pROC::auc(testAll$y, res, ))

    # select new indexes which will be added to the training set
    newIdx <- get_new_idx(trainAll, train, fit, k)

    trainNew <- inner_join(trainAll, newIdx, by = "row_id")
    train    <- bind_rows(train, trainNew)
    
    # remove selected indexes from available 'unlabelled' set.
    trainAll <- anti_join(trainAll, newIdx,by = "row_id") 
  }
  
  return(aucRes)
}

get_new_idx_random <- function(trainAll, train, fit, k) {
  trainAll %>% sample_n(k, replace = FALSE) %>% select(row_id)
}

make_plot <- function(result, addRibbon = FALSE) {
  result2 <- result %>% 
    group_by(round, Type) %>%
    summarise(
      AUC_Mean = mean(AUC),
      SD = sd(AUC),
      q025 = quantile(AUC, probs = 0.025),
      q975 = quantile(AUC, probs = 0.975))
  
  p <- ggplot(result2 %>% mutate(round = as.integer(round))) +
    geom_line(aes(round, AUC_Mean, color = Type), size = 1.5) + 
    theme_bw()

  if(addRibbon) {
    p <- p +
      geom_ribbon(aes(round, ymax = q025, ymin = q975, fill = Type), alpha = 0.2)
  }

  return(p)
}

transform_run <- function(x) {
  xx <- t(x)
  colnames(xx) <- 1:ncol(xx)
  rownames(xx) <- 1:nrow(xx)
  res <- bind_cols(tibble(iter = 1:nrow(xx)), as.data.frame(xx))
  res <- pivot_longer(res, cols = c(-iter), names_to = "round", values_to = "AUC")
  res
}

set.seed(123)
allRandom <- pbapply::pbreplicate(
  50,
  make_active_learning_path(trainAll, testAll, get_new_idx = get_new_idx_random)
  ) %>% transform_run %>% mutate(Type = "0. Random")

```

```{r}

set.seed(123)
entrRes <- pbapply::pbreplicate(
  50,
  make_active_learning_path(trainAll, testAll, get_new_idx = get_new_idx_entropy)
) %>% transform_run %>% mutate(Type = "1. Entropy")


```

The result is not bad (I know that the difference is not big - 1 percentage point in AUC , but still I'm happy with this result;)). At the beginning, the achieved a gain in performance is better for the entropy-based algorithm. However, it quickly becomes fully exploited and in the long run achieved performance gain is worse than in the random strategy.

```{r}
make_plot(bind_rows(allRandom, entrRes))
```

One note - one might think this solution was more on 'exploration', not exploitation. But this is not the case. It exploits a specific property of the sample - in this case its entropy. However, exploration should try different things. 

## Second experiment - entropy scaled by information gain.

Building on entropy I created one another experiment. Here I'm weighting the variable entropy by its information gain. I'm not sure that it makes sense, but intuitively it should promote those variables which have good signal in them. So, let's do the simulation.

```{r}
######### entropy2
get_new_idx_entropy_information_gain <- function(trainAll, train, fit, k) {

  tt <- suppressWarnings(train %>% select(-row_id) %>% discretize(y ~ .) %>% extract_discretize_transformer())
  dt <- trainAll %>% discretize_transform(disc = tt) %>% select(-y)
  tr <- discretize_transform(disc = tt, data = train) %>% select(-y)

  colNames <- head(colnames(dt),-1)

  infGain <- information_gain(y ~ ., train %>% select(-row_id))

  res <- setNames(lapply(colNames, function(nm) {
    ent <- FSelectorRcpp:::fs_entropy1d(tr[[nm]])
    rr <- sapply(unique(dt[[nm]]), function(x) {
      setNames(FSelectorRcpp:::fs_entropy1d(c(tr[[nm]], x)) - ent, as.character(x))
    }, USE.NAMES = FALSE)

    rr <- rr * (infGain %>% filter(attributes == nm))$importance
    rr <- tibble(nm = names(rr), value = rr)
    colnames(rr)[1] <- nm
    rr
  }), colNames)

  dt2 <- dt
  for(ii in res) {
    dt2 <- inner_join(dt2, ii, by = colnames(ii)[[1]])
  }

  dt2 <- dt2 %>% mutate(entrChange = rowSums(dt2 %>% select(contains("value"))))
  dt2 %>% arrange(desc(entrChange)) %>% select(row_id) %>% head(k)
}
```

```{r}

set.seed(123)
entrResWithInfGain<- pbapply::pbreplicate(
  50,
  make_active_learning_path(
    trainAll, testAll, 
    get_new_idx = get_new_idx_entropy_information_gain)
) %>% transform_run %>% mutate(Type = "2. Entropy with information gain")


```
But, the results is disappointing. See the plot below. It's worse than random and the first version of the entropy-based strategy. This  approach tried to exploit a very specific property of the sample. It's like trying to predict  which variable will be the best predictors, without a model  or a good theory. 

```{r}
make_plot(bind_rows(allRandom, entrRes, entrResWithInfGain))

```

## Summary

The conclusion is very similar to those presented in Part I. Handcrafted solutions, which might intuitively sound reasonable, might occur to be worse than just doing nothing. The moral is simple, to achieve good results one need do the homework and use proper theory.
